{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords e simbolos a retirar\n",
    "stop_words = set(['de','a','o','que','e','é','do','da','em','um','para','com','não','uma','os','no','se','na','por','mais','as','dos','como','mas','ao','ele','das','à','seu','sua','ou','quando','muito','nos','já','eu','também','só','pelo','pela','até','isso','ela','entre','depois','sem','mesmo','aos','seus','quem','nas','me','esse','eles','você','essa','num','nem','suas','meu','às','minha','numa','pelos','elas','qual','nós','lhe','deles','essas','esses','pelas','este','dele','tu','te','vocês','vos','lhes','meus','minhas','teu','tua','teus','tuas','nosso','nossa','nossos','nossas','dela','delas','esta','estes','estas','aquele','aquela','aqueles','aquelas','isto','aquilo','estou','está','estamos','estão','estive','esteve','estivemos','estiveram','estava','estávamos','estavam','estivera','estivéramos','esteja','estejamos','estejam','estivesse','estivéssemos','estivessem','estiver','estivermos','estiverem','hei','há','havemos','hão','houve','houvemos','houveram','houvera','houvéramos','haja','hajamos','hajam','houvesse','houvéssemos','houvessem','houver','houvermos','houverem','houverei','houverá','houveremos','houverão','houveria','houveríamos','houveriam','sou','somos','são','era','éramos','eram','fui','foi','fomos','foram','fora','fôramos','seja','sejamos','sejam','fosse','fôssemos','fossem','for','formos','forem','serei','será','seremos','serão','seria','seríamos','seriam','tenho','tem','temos','tém','tinha','tínhamos','tinham','tive','teve','tivemos','tiveram','tivera','tivéramos','tenha','tenhamos','tenham','tivesse','tivéssemos','tivessem','tiver','tivermos','tiverem','terei','terá','teremos','terão','teria','teríamos','teriam'])\n",
    "\n",
    "symbols = set(['-', 'r$', 'R$', '.'])\n",
    "\n",
    "# Doria\n",
    "# separando as palavras\n",
    "palavras = \"\".join(politicos['Doria']['text']).lower().split()\n",
    "# returando stopwords e depois os web links\n",
    "palavras = [w for w in palavras if not w in (stop_words | symbols)]\n",
    "palavras = [w for w in palavras if re.match(r'https://*', w) == None]\n",
    "\n",
    "c = Counter(palavras)\n",
    "# retirando palavras com frequencia menor do que 5 e adicionando número ao candidato\n",
    "dados = {(1,x,y) for x,y in zip(c,c.values()) if y > 5}\n",
    "print(\"Doria\", len(dados))\n",
    "\n",
    "# Bolsonaro\n",
    "palavras = \"\".join(politicos['Bolsonaro']['text']).lower().split()\n",
    "palavras = [w for w in palavras if not w in (stop_words | symbols)]\n",
    "palavras = [w for w in palavras if re.match(r'https://*', w) == None]\n",
    "\n",
    "c = Counter(palavras)\n",
    "d = {(2,x,y) for x,y in zip(c,c.values()) if y > 5}\n",
    "print(\"Bolsonaro\", len(d))\n",
    "# adicionando aos dados\n",
    "for x in d:\n",
    "    dados.add(x)\n",
    "\n",
    "# Lula\n",
    "palavras = \"\".join(politicos['Lula']['text']).lower().split()\n",
    "palavras = [w for w in palavras if not w in (stop_words | symbols)]\n",
    "palavras = [w for w in palavras if re.match(r'https://*', w) == None]\n",
    "\n",
    "c = Counter(palavras)\n",
    "d = {(3,x,y) for x,y in zip(c,c.values()) if y > 5}\n",
    "print(\"Lula\", len(d))\n",
    "for x in d:\n",
    "    dados.add(x)\n",
    "\n",
    "print(\"Total\", len(dados))\n",
    "\n",
    "# montando o dataframe\n",
    "df = pd.DataFrame(dados, columns=['candidato', 'texto', 'frequencia'])\n",
    "df.sort_values('frequencia', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_unicas = set()\n",
    "numero_total_palavras = 0\n",
    "for c in palavras:\n",
    "    for p in c:\n",
    "        palavras_unicas.add(p)\n",
    "        numero_total_palavras += 1\n",
    "print(numero_total_palavras, len(palavras_unicas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Bag of word sentence 1:\n",
      "{'likes': 2, 'movies': 2, 'john': 1, 'to': 1, 'watch': 1, 'mary': 1, 'too': 1}\n",
      "We found 7 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too.\"]\n",
    "\n",
    "def print_bow(sentence: str) -> None:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence)\n",
    "    sequences = tokenizer.texts_to_sequences(sentence)\n",
    "    word_index = tokenizer.word_index \n",
    "    bow = {}\n",
    "    for key in word_index:\n",
    "        bow[key] = sequences[0].count(word_index[key])\n",
    "\n",
    "    print(f\"Bag of word sentence 1:\\n{bow}\")\n",
    "    print(f'We found {len(word_index)} unique tokens.')\n",
    "\n",
    "print_bow(sentence)"
   ]
  }
 ]
}